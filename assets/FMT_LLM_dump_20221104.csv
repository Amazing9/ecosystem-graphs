Name,Announced date,Languages,Organization,Paper / announcement link,All sizes,Train data?,Model?,Code?,Release Grade,API Comments,API?,Additional comments,Code URL,Code release comments,Eval artifacts release comments,Eval artifacts?,Eval datasets,Language comments,Params (B; Max),Model release comments,Training artifacts release comments,Type,Cluster,Code License,Created time,Last edited time,Model License,Model Type Comment,Model artifacts link,Significant?,Training artifacts link,_HIDEME
OPT,2022/05/01,English,FAIR,https://arxiv.org/abs/2205.01068,0.125B; 0.350B; 1.3B; 2.7B; 6.7B; 13B; 30B; 175B,🔶,✅,✅,B+,,,,https://github.com/facebookresearch/metaseq,,Awaiting prediction logits release,❌,,,175,All released except 66B (TBD) and 17B (requires manual approval),"Datasets - Public
Scripts - Awaiting release",LLM,FAIR cluster (124 * DGX A100 [8x80GB A100]),,"May 6, 2022 8:34 PM","October 16, 2022 7:59 PM",OPT-175B LICENSE,,,🎆,,
ERNIE 3.0,2021/07/05,"Chinese | 中文, English",Baidu,https://arxiv.org/abs/2107.02137,10B,❌,❌,❌,F,,,,,Previous versions available at https://github.com/PaddlePaddle/ERNIE,,❌,,"Separate English and Chinese models, not multilingual",10,,,LLM,,,"May 6, 2022 8:34 PM","October 16, 2022 7:59 PM",,,,🎆,,
JURASSIC,2021/08/11,English,AI21,https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf,7.5B; 178B,🔶,❌,❌,D,,✔️,,,,https://github.com/ai21labs/lm-evaluation,🔶,,,178,,Paper says training data is from public sources but does not list which ones. No scripts provided.,LLM,,,"May 6, 2022 8:34 PM","October 16, 2022 7:59 PM",,,,🎆,,
GPT-NeoX,2022/04/14,English,EleutherAI,https://arxiv.org/abs/2204.06745,20B,✅,✅,✅,B+,,,,https://github.com/EleutherAI/gpt-neox,,Trained on the Pile. Replication code for the Pile is available. See https://arxiv.org/abs/2101.00027,🔶,,,20,"In addition to final checkpoint weights, also released checkpoints throughout training",,LLM,,,"May 6, 2022 9:21 PM","October 16, 2022 7:59 PM",Apache 2.0,,,🎆,,
PaLM,2022/04/05,English,Google,https://arxiv.org/abs/2204.02311,540B,❌,❌,❌,F,,,,,,,❌,,,540,,,LLM,6144 TPU v4,,"May 6, 2022 9:18 PM","October 16, 2022 7:59 PM",,,,🎆,,
GPT-3,2020/05/28,English,OpenAI,https://arxiv.org/abs/2005.14165,0.125B; 0.350B; 0.760B; 1.3B; 2.7B; 6.7B; 13B; 175B,❌,❌,❌,F,,✔️,,,,,❌,,,175,,,LLM,,,"May 6, 2022 9:06 PM","October 16, 2022 7:59 PM",,,,🎆,,
Chinchilla,2022/03/29,English,DeepMind,https://arxiv.org/abs/2203.15556,70B,❌,❌,❌,F,,,,,,,❌,,,70,,,LLM,,,"May 6, 2022 9:06 PM","October 16, 2022 7:59 PM",,,,🎆,,
Gopher,2021/12/08,English,DeepMind,https://arxiv.org/abs/2112.11446,0.044B; 0.117B; 0.417B; 1.4B; 7.1B; 280B,❌,❌,❌,F,,,,,,,❌,,,280,,,LLM,,,"May 6, 2022 9:06 PM","October 16, 2022 7:59 PM",,,,🎆,,
Megatron-LM,2021/04/09,English,NVIDIA,https://arxiv.org/abs/2104.04473 (1T model) https://arxiv.org/abs/1909.08053,1.7B; 3.6B; 7.5B; 18B; 39B; 76B; 145B; 310B; 530B; 1000B,✅,❌,✅,C,,,,https://github.com/NVIDIA/Megatron-LM#,,,❌,,,1000,,(Assuming the 1T model shares data pipeline with other models in family),LLM,,,"May 6, 2022 9:06 PM","October 16, 2022 7:59 PM",Megatron-LM License (modified Apache 2.0),,Weights for 1T model not released,🎆,,
T5,2019/10/23,English,Google,https://arxiv.org/abs/1910.10683,0.060B; 0.220B; 0.770B; 3B; 11B,✅,✅,✅,B+,,,,https://github.com/google-research/text-to-text-transfer-transformer,,,❌,,,11,Checkpoints for 0.060B; 0.220B; 0.770B; 3B; 11B,,LLM,,,"May 6, 2022 9:06 PM","October 16, 2022 7:59 PM",Apache 2.0,,https://github.com/google-research/text-to-text-transfer-transformer,🎆,https://www.tensorflow.org/datasets/catalog/c4,
Switch-Transformer,2021/01/11,English,Google,https://arxiv.org/abs/2104.12369,7B; 26B; 395B; 1571B,✅,❌,✅,C,,,,https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py,,,❌,,,1571,,,LLM; Sparse,,,"May 6, 2022 9:05 PM","October 16, 2022 7:59 PM",,,,🎆,https://www.tensorflow.org/datasets/catalog/c4,
Megatron-Turing NLG,2022/01/28,English,"Microsoft, NVIDIA",https://arxiv.org/abs/2201.11990,530B,🔶,✅,✅,C-,,,,,,,❌,,,530,"Weights for BERT-345M released, unable to find checkpoints for larger models",Datasets available but filtering code not released,LLM,,,"May 6, 2022 8:46 PM","October 16, 2022 7:59 PM",Megatron-LM License (modified Apache 2.0),,See https://huggingface.co/docs/transformers/model_doc/megatron-bert,🎆,,
PanGu-α,2021/04/26,Chinese | 中文,Huawei,https://arxiv.org/abs/2104.12369,2.6B; 13B; 200B,🔶,❌,❌,F,,,,,,,❌,,,200,,"Data sources are public, but authors used extensive filtering and cleaning",LLM,,,"May 6, 2022 8:46 PM","October 16, 2022 7:59 PM",,,,🎆,,
T0,2021/10/15,English,BigScience,"https://arxiv.org/abs/2110.08207 ",3B; 11B,✅,✅,✅,B+,,✔️,The highest quality release,https://github.com/bigscience-workshop/t-zero,,,🔶,,,11,Also released ablation checkpoints,https://github.com/bigscience-workshop/t-zero/blob/master/training/README.md,LLM,,,"May 6, 2022 8:46 PM","October 16, 2022 7:59 PM",Apache 2.0,,,🎆,,
Yuan,2021/10/10,Chinese | 中文,Inspur,https://arxiv.org/abs/2110.04725,245B,❌,❌,❌,F,,,,,,,❌,,,245,,,LLM,,,"May 6, 2022 8:46 PM","October 16, 2022 7:59 PM",,,,🎆,,
HyperCLOVA,2021/05/21,Korean | 한국어,Naver,https://arxiv.org/abs/2109.04650,0.137B; 0.350B; 0.760B; 1.3B; 6.9B; 13B; 39B; 82B,❌,❌,❌,F,HyperCLOVA Studio announced but not released.,🔶,Announced 204B parameter model http://m.koreaherald.com/view.php?ud=20210525000824,,,,❌,,,82,,"Appendix A.1 describes collection process, but no data release.",LLM,,,"May 6, 2022 8:46 PM","October 16, 2022 7:59 PM",,,,🎆,,
Wu Dao,2021/07/14,Chinese | 中文,BAAI,None,1750B,❌,❌,❌,Z-,,,"No paper, only a press release - https://www.tsinghua.edu.cn/en/info/1245/10472.htm",,,,❌,,,1750,,,LLM,,,"May 6, 2022 8:45 PM","October 16, 2022 7:59 PM",,,,🎆,,
XGLM,2021/12/20,A lot of languages,FAIR,https://arxiv.org/abs/2112.10668,0.564B; 1.7B; 2.9B; 7.5B,🔶,✅,✅,C,,,,https://github.com/facebookresearch/fairseq/blob/main/examples/xglm/README.md,,,❌,,,7.5,,"“XGLM models are trained on a new multilingual corpus extracted from CommonCrawl (CC100-XL), a significantly larger multilingual dataset covering 68 Common Crawl (CC) snapshots (from http://commoncrawl.org/2013/11/new-crawl-data-available/ to https://commoncrawl.org/2020/04/march-april-2020-crawl-archive-now-available/ consisting of 134 languages. The detailed languages and data statistics are reported in the paper (Table A.1).”",LLM (Multilingual),,,"May 6, 2022 10:27 PM","October 16, 2022 7:59 PM",MIT,,https://github.com/facebookresearch/fairseq/blob/main/examples/xglm/README.md,🎆,,
GPT-SW3,2022/02/25,Swedish | Svenska,AI-Nordics,http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf,3.5B,✅,🔶,✅,F,,,No paper. Description at https://huggingface.co/AI-Nordics/gpt-sw3. Announcement https://twitter.com/Sofie_Hvitved/status/1497133862442393626,https://github.com/NVIDIA/Megatron-LM,Uses Megatron-LM code,,❌,,,3.5,Approval required. Restrictive license.,,LLM,,,"May 6, 2022 10:27 PM","October 16, 2022 7:58 PM",,,,No,,
T-Few,2022/05/11,English,UNC-Chapel Hill (Colin Raffel Group),https://arxiv.org/abs/2205.05638,3B; 11B,✅,✅,✅,B+,,,,https://github.com/r-three/t-few,,,❌,,,11,,,LLM,,,"May 12, 2022 3:20 PM","October 16, 2022 7:59 PM",MIT,"Finetuning of T0, which was finetuned from T5",https://github.com/r-three/t-few,🎆,,
UL2,2022/05/10,English,Google,https://arxiv.org/abs/2205.05131,20B,✅,✅,🔸,B+,,,,https://github.com/google-research/google-research/tree/master/ul2,"See Appendix 9.1. AFAICT, actual code used for training not released, but libraries and relevant snippets are. ",,❌,,,20,,,LLM,128 TPUv4,,"May 12, 2022 3:13 PM","October 16, 2022 7:59 PM",?,,https://github.com/google-research/google-research/tree/master/ul2,🎆,https://www.tensorflow.org/datasets/catalog/c4,
PAGNol,2021/05/04,French | français,LightOn,https://arxiv.org/abs/2110.08554,0.124B; 0.355B; 0.773B; 1.5B,✅,❌,❌,D,,,Trained on JeanZay,,,,❌,,,1.5,,CCNet and OSCAR. No additional filtering indicated,LLM,,,"May 7, 2022 7:15 PM","October 16, 2022 7:58 PM",,,,No,,
Cedille,2021/11/10,French | français,Coteries,https://arxiv.org/abs/2202.03371,6B,🔶,✅,✅,B,https://cedille.ai/,✔️,Announced on reddit,https://github.com/coteries/cedille-ai,Trained using mesh-transformer-jax,,❌,,,6,,Extensive filtering of multilingual C4 for French data. Scripts not available,LLM,,,"May 7, 2022 7:08 PM","October 16, 2022 7:58 PM",MIT,,https://github.com/coteries/cedille-ai,No,,
Anthropic Unnamed 52B,2021/12/01,English,Anthropic,,0.013B; 0.042B; 0.197B; 0.810B; 2.7B; 13B; 52B,❌,❌,❌,F,,,,,,,❌,,,52,,,LLM,,,"May 7, 2022 6:46 PM","October 16, 2022 7:59 PM",,,,🎆,,
LaMDA,2022/01/20,English,Google,https://arxiv.org/abs/2201.08239,2B; 8B; 137B,❌,❌,❌,,,,,,,,❌,,,137,,"The pre-training data, called Infiniset, is a combination of dialog data from public dialog data and other public web documents… (Appendix F)",LLM; Dialogue,,,"July 8, 2022 2:18 PM","October 16, 2022 7:59 PM",,,,🎆,,
YaLM,2022/06/22,"English, Russian | Русский",Yandex,https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6,100B,❌,✅,❌,,,,,https://github.com/yandex/YaLM-100B,“the code in this repo is not the same code that was used to train the model. Rather it is stock example from DeepSpeed repo with minimal changes needed to infer our model.”,No evaluations described in Medium article,❌,,"25% Pile (mostly English), 75% Russian ",100,,,LLM,Yandex 800 A100 Cluster,,"July 8, 2022 2:19 PM","October 16, 2022 7:59 PM",Apache 2.0,,https://github.com/yandex/YaLM-100B,🎆,,
Minerva,2022/06/29,English,Google,https://arxiv.org/abs/2206.14858,8B; 62B; 540B,❌,❌,✅,,,,,https://github.com/google-research/t5x,We used the t5x framework and trained our models with v4 TPU on Google Cloud,,❌,,,540,,,LLM; Math,,,"July 8, 2022 2:33 PM","October 16, 2022 7:59 PM",,,,🎆,,
NLLB,2022/07/06,,FAIR,,,,,,,,,,,,,,,,,,,LLM,,,"July 8, 2022 2:41 PM","July 8, 2022 2:43 PM",,,,,,TRUE
DeepNet,2022/03/01,English,Microsoft,https://arxiv.org/abs/2203.00555,,,,,,,,,,,,,,,,,,LLM,,,"July 8, 2022 2:55 PM","July 8, 2022 4:04 PM",,,,,,TRUE
GPT-4chan,2022/06/03,English,Yannic Kilcher,(video) https://youtu.be/efPrtcLdcdM,,,,,,,,,,,,,,,,,,LLM,,,"July 8, 2022 4:55 PM","July 8, 2022 4:57 PM",,,,,,TRUE
KoGPT,2021/11/15,Korean | 한국어,KakaoBrain,,1.5B,❌,✅,✅,,,,,https://github.com/kakaobrain/kogpt,,,❌,,,1.5,,"Did not find a more specific reference than “KakaoBrain KoGPT was trained on raw data, a dataset known to contain profanity, lewd, political changed, and other harsh language”",LLM,,,"July 11, 2022 4:56 PM","October 16, 2022 7:58 PM",CC-BY-NC-ND 4.0,,https://github.com/kakaobrain/kogpt,No,,
BLOOM,2022/07/12,"A lot of languages, Code | 1 0, English",BigScience,,176B,🔶,✅,✅,,,,,https://github.com/bigscience-workshop/Megatron-DeepSpeed,Fork of Megatron-DeepSpeed which is fork of Megatron-LM,"See https://github.com/bigscience-workshop/evaluation-results. 

Model predictions are not provided, see https://www.notion.so/8e6e317621ea4339a035a113958c8a12 ",🔶,,See https://huggingface.co/bigscience/bloom#training-data,176,Model is licensed under https://huggingface.co/spaces/bigscience/license,Haven’t found an easy way to download all datasets within the Big Science Corpus. Each dataset can have a different process to obtain and license,LLM,Jean Zay (48 * 8xA100 80GB nodes),,"July 12, 2022 4:03 PM","October 16, 2022 7:59 PM",BigScience RAIL v1.0,,https://huggingface.co/bigscience/bloom,🎆,https://huggingface.co/spaces/bigscience/BigScienceCorpus,FALSE
BlenderBot 3,2022/08/05,English,FAIR,https://github.com/facebookresearch/ParlAI/blob/main/projects/bb3/BB3_main_tech_report.pdf,3B; 30B; 175B,✅,🔶,✅,,https://www.blenderbot.ai/,✔️,,https://parl.ai/projects/bb3/#code,,Many human evaluations were conducted. Note that most LLM papers are not evaluated as chat bots and not evaluated by humans. Updating the grading rubric to take note of this distinction.,,,,175,"3B and 30B models available for download, 175B model gated behind a form https://docs.google.com/forms/d/e/1FAIpQLSfRzw8xVzxaxgRyuodTZtkcYADAjzYjN5gcxx6DMa4XaGwwhQ/viewform",Note that BB3 is finetuned on top of https://www.notion.so/OPT-2cfe13ca04994ab8be9b8cf6761d3981,LLM; Dialogue,,,"August 5, 2022 9:04 PM","October 16, 2022 7:59 PM",BB3-175B LICENSE,,https://parl.ai/projects/bb3/#code,🎆,https://parl.ai/projects/bb3/#datasets,FALSE
GLM-130B,2022/08/04,"Chinese | 中文, English",Tsinghua (THUDM),(TBA) - https://keg.cs.tsinghua.edu.cn/glm-130b/,130B,🔶,🔶,✅,,https://huggingface.co/spaces/THUDM/GLM-130B,✔️,,https://github.com/THUDM/GLM-130B,,,❌,"LAMBADA, MMLU","Bilingual model - about half English, half Chinese data.",130,"Available but gated on approval and license, see https://www.notion.so/bc73f382871749a39cd9234799affda9 ",Trained on Pile (public) and “Chinese Corpora” unspecified and not yet released,LLM,THUDM 96 DGX-A100 (40G) cluster,Apache 2.0 https://github.com/THUDM/GLM-130B/blob/main/LICENSE,"August 6, 2022 9:38 PM","October 16, 2022 7:59 PM",GLM-130B License,,https://docs.google.com/forms/d/e/1FAIpQLSehr5Dh_i3TwACmFFi8QEgIVNYGmSPwV0GueIcsUev0NEfUug/viewform,🎆,,
Sparrow,2022/09/20,English,DeepMind,https://storage.googleapis.com/deepmind-media/DeepMind.com/Authors-Notes/sparrow/sparrow-final.pdf,70B,❌,❌,❌,,,,,,,,❌,,,70,Based on Chinchilla-70B,,LLM; Dialogue,,,"August 31, 2022 11:15 AM","October 16, 2022 7:59 PM",,,,🎆,,
Polyglot-Ko,2022/09/27,Korean | 한국어,EleutherAI,(none) https://github.com/EleutherAI/gpt-neox,1.3; 3.8; 6.7,❌,✅,✅,,,,"“We decided to show only COPA and HellaSwag from KOBEST because evaluated models performed similarly to random guesses or with high variance on other tasks.” - outperforms kakaobrain kogpt, FAIR xglm on those two tasks",https://github.com/EleutherAI/gpt-neox,,,❌,KoBEST,,6.7,6.7B forthcoming,"Unnamed Korean corpus by TUNiB. ",LLM,Stability.AI,Apache 2.0,"September 27, 2022 11:08 AM","October 16, 2022 7:59 PM",Apache 2.0,,https://github.com/eleutherai/polyglot,🎆,,
CodeGeeX,2022/09/20,Code | 1 0,Tsinghua (THUDM),https://github.com/THUDM/CodeGeeX,13B,✅,✅,✅,,,,,https://github.com/THUDM/CodeGeeX,,,,HumanEval-X,,13,Not yet available,The Pile and CodeParrot,LLM; Code,THUDM 1536 Ascend 910 (32GB) Cluster,Apache 2.0,"September 29, 2022 9:47 PM","October 16, 2022 7:59 PM",?,,,🎆,"https://pile.eleuther.ai/
https://github.com/huggingface/transformers/tree/main/examples/research_projects/codeparrot",
FIM-NeoX-1.3B,2022/10/07,English,CarperAI,https://huggingface.co/CarperAI/FIM-NeoX-1.3B,1.3B,✅,✅,✅,,,,First model released by Carper AI,https://github.com/EleutherAI/gpt-neox,,,,,,1.3,,,LLM,Stability.AI,Apache 2.0,"October 16, 2022 7:27 PM","October 16, 2022 7:57 PM",Apache 2.0,,https://huggingface.co/CarperAI/FIM-NeoX-1.3B,No,https://github.com/EleutherAI/the-pile,
CPM-Generate,2020/12/01,Chinese | 中文,"BAAI, Tsinghua (THUDM)",https://arxiv.org/abs/2012.00413,0.109; 0.334; 2.6,❌,✅,✅,,,,,https://github.com/TsinghuaAI/CPM,,,❌,,,2.6,,Unable to locate training data,LLM,Tsinghua 64 V100,MIT,"October 16, 2022 7:32 PM","October 16, 2022 7:59 PM",,,https://github.com/TsinghuaAI/CPM,🎆,,
GPT-Neo-Japanese-1.3B,2021/12/08,Japanese | 日本語,Yellowback,https://tech.yellowback.net/posts/gpt-neo-japanese,1.3B,✅,✅,✅,,,,,,Assuming GPT Neo codebase was used,,,,,1.3,,"CC100-ja, Oscar-ja, Wiki-ja",LLM,,Apache 2.0,"October 16, 2022 8:27 PM","October 16, 2022 8:34 PM",?,,,🎆,https://huggingface.co/yellowback/gpt-neo-japanese-1.3B,
CPM-2,2021/06/20,"Chinese | 中文, English","BAAI, Tsinghua (LHH)",https://arxiv.org/abs/2106.10715,11B,❌,✅,✅,,,,,https://github.com/TsinghuaAI/CPM,,,,CUGE,"2.3TB Chinese, 300GB English",11,,,LLM,BAAI cluster,MIT,"October 16, 2022 8:35 PM","October 16, 2022 8:52 PM",,,https://github.com/TsinghuaAI/CPM,🎆,,
Japanese-GPT,2022/01/24,Japanese | 日本語,Rinna,n/a,0.037B; 0.110B; 0.336B; 1.3B,✅,✅,✅,,,,,https://github.com/rinnakk/japanese-pretrained-models,,,❌,,,1.3,,,LLM,,Apache 2.0,"October 17, 2022 1:45 PM","October 17, 2022 1:49 PM",?,,https://github.com/rinnakk/japanese-pretrained-models,,"CC-100-ja, Wikipedia-ja",
gpt-neox-japanese-2.7b,2022/06/16,Japanese | 日本語,ABEJA,https://medium.com/ml-abeja/training-a-better-gpt-2-93b157662ae4,2.7B,✅,✅,✅,,,,Artifacts were uploaded on 08/29 to huggingface,,“GPT-NeoX-Based” - assuming they used the same repo,,❌,,,2.7,,,LLM,,MIT,"October 17, 2022 1:53 PM","October 17, 2022 2:01 PM",MIT,,https://huggingface.co/abeja/gpt-neox-japanese-2.7b,,"http://data.statmt.org/cc-100/ja.txt.xz, https://dumps.wikimedia.org/other/cirrussearch, https://huggingface.co/datasets/oscar",
Tabnine Unnamed Code Models,2022/06/15,Code | 1 0,Tabnine,https://www.tabnine.com/blog/announcing-tabnine-next-generation/,400M; …; 3B,❌,❌,❌,,Paid code completion product. See https://www.tabnine.com/,✔️,Sizes per CEO: https://twitter.com/drorwe/status/1539293085112344578?s=20&t=khquSTAtK6q4sPAuGSRTqA,,,,❌,,,3,,,LLM; Code,,,"October 17, 2022 4:23 PM","October 17, 2022 4:26 PM",,,,,,
Flan-PaLM 540B,2022/10/20,"A lot of languages, English",Google,https://arxiv.org/abs/2210.11416,540B,🔶,❌,❌,,,,512 v4 TPU chips for 37 hours,,,,❌,,60 total languages across finetuning datasets,540,,Existing datasets are public. CoT annotations are not.,LLM,,,"October 20, 2022 11:45 PM","November 4, 2022 1:02 PM",,,,🎆,,
Flan-T5,2022/10/20,"A lot of languages, English",Google,https://arxiv.org/abs/2210.11416,.080; .250; .780; 3; 11,✅,✅,✅,,,,,https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints,,,❌,,60 total languages across finetuning datasets,11,,,LLM,,,"October 20, 2022 11:47 PM","November 4, 2022 1:02 PM",Apache 2.0,,https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints,,,