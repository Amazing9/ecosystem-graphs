---

# models

- type: model
  name: Megatron-Turing NLG
  organization: NVIDIA
  description: Megatron-Turing NLG is a 530B parameter autoregressive large language
    model.
  created_date:
    value: 2022-01-28
    explanation: The date the model paper was submitted to Arxiv
  url: https://arxiv.org/abs/2201.11990
  model_card: ''
  modality: Text (English)
  size: 530B parameters (dense model)
  analysis: ''
  dependencies:
    - Pile-Books3
    - Pile-OpenWebText2
    - Pile-Stack Exchange
    - Pile-PubMed
    - Pile-Wikipedia
    - Pile-Gutenberg
    - Pile-BookCorpus2
    - Pile-NIH
    - Pile-ArXiv
    - Pile-GitHub
    - Pile-CC
    - Pile-CommonCrawl
    - Pile-Realnews
    - Pile-CC-Stories
  training_emissions: ''
  training_time: ''
  training_hardware: 4480 A100s (560 x 8)
  quality_control: ''
  access:
    value: Limited public access
    explanation: Megatron-Turing NLG can be accessed through the [[Turing Academic
      Program]](https://www.microsoft.com/en-us/research/collaboration/microsoft-turing-academic-program/)
  license:
    value: n/a
    explanation: n/a
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''

- type: model
  name: Megatron-LM
  organization: NVIDIA
  description: Megatron-LM is an autoregressive language model
  created_date:
    value: 2021-04-09
    explanation: The date the paper for the 1 trillion parameter model was published
  url: https://arxiv.org/abs/2104.04473
  model_card: None
  modality: Text (English)
  analysis: ''
  size: 1000B parameters (dense model)
  dependencies: []
  training_emissions: Unknown
  training_time: 84 days
  training_hardware: 3072 A100 GPUs
  quality_control: Unknown
  access:
    value: No public access
    explanation: >
      Neither the 8.3B parameter model trained to convergence nor the 1 trillion
      paramter model is available for download
  license:
    value: n/a
  intended_uses: None
  prohibited_uses: None
  monitoring: None
  feedback: None
