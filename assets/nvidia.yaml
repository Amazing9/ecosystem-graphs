---

# models

- type: model
  name: Megatron-Turing NLG
  organization: NVIDIA
  description: Megatron-Turing NLG is a 530B parameter autoregressive large language model.
  created_date:
    value: 2022-01-28
    explanation: The date the model paper was submitted to Arxiv
  url: https://arxiv.org/abs/2201.11990
  model_card: ''
  modality: Text (English)
  size: 530B parameters (dense model)
  analysis: ''
  dependencies:
    - Pile-Books3
    - Pile-OpenWebText2
    - Pile-Stack Exchange
    - Pile-PubMed
    - Pile-Wikipedia
    - Pile-Gutenberg
    - Pile-BookCorpus2
    - Pile-NIH
    - Pile-ArXiv
    - Pile-GitHub
    - Pile-CC
    - Pile-CommonCrawl
    - Pile-Realnews
    - Pile-CC-Stories
  training_emissions: ''
  training_time: ''
  training_hardware: 4480 A100s (560 x 8)
  quality_control: ''
  access: No public access
  license:
    value: n/a
    explanation: n/a
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
