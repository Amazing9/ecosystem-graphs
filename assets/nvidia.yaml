---

# models
- type: model
  name: Megatron-LM
  organization: NVIDIA
  description: Megatron-LM is an autoregressive language model
  created_date:
    value: 2021-04-09
    explanation: The date the paper for the 1 trillion parameter model was published
  url: https://arxiv.org/abs/2104.04473
  model_card: None
  modality: Text (English)
  analysis: ''
  size: 1000B parameters (dense model)
  dependencies: []
  training_emissions: Unknown
  training_time: 84 days
  training_hardware: 3072 A100 GPUs
  quality_control: Unknown
  access:
    value: Closed
    explanation: >
      Neither the 8.3B parameter model trained to convergence nor the 1 trillion
      paramter model is available for download
  license:
    value: Unknown
  intended_uses: None
  prohibited_uses: None
  monitoring: None
  feedback: None

- type: dataset
  name: MineDojo
  organization: NVIDIA
  description: ''
  created_date:
    value: 2022-06-17
  url: https://arxiv.org/abs/2206.08853
  datasheet: ''
  modality: Videos, Text
  size: 730k videos, 6k Wikipedia pages, 340k reddit posts
  sample: ''
  analysis: ''
  dependencies: [YouTube, Wikipedia, Reddit]
  included: ''
  excluded: ''
  quality_control: ''
  access: Open
  license: MIT License
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
