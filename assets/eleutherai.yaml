---

- type: dataset
  name: The Pile
  # General
  organization: EleutherAI
  description: >
    A latge language model training dataset, used to train GPT-NeoX-20B.
  created_date: "2021-01-01"
  url: https://arxiv.org/pdf/2101.00027.pdf
  datasheet: https://arxiv.org/pdf/2201.07311.pdf
  modality: Text (English) and code
  size: 825 GB
  sample:
  - ...pot trending topics and the coverage around them. First up, there’s a
    bit of a visual redesign. Previously, clicking on a trending topic would
    highlight a story from one publication, and you’d have to scroll down past
    a live video section to view related stories. Facebook is replacing that
    system with a simple carousel, which does a better job of showing you
    different coverage options. To be clear, the change doesn’t affect how
    stories are sourced, according to Facebook. It’s still the same algorithm
    pickin...
  - Total knee arthroplasty (TKA) is a promising treatment for endstage
    osteoarthritis (OA) of the knee for alleviating pain and restoring the
    function of the knee. Some of the cases with bilateral TKA are symptomatic,
    necessitating revision arthroplasty in both the knees. A bilateral revision
    TKA can be done ei
  - On the converse, the set-valued map $\Phi:[0,3]\rightrightarrows [0,3]$
    $$\Phi(x):=\left\{\begin{array}{ll} \{1\} & \mbox{ if } 0\leq x<1\\ {}[1,2]
    & \mbox{ if } 1\leq x\leq 2\\ \{2\} &
  - This Court thus uses the same interpretation of V.R.C.P. 52(a) as it did
    *487 under the previous statutory requirement found in 12 V.S.A. §
    2385.  In essense, the defendants urge that this Court should reconsider
    the case of Green Mountain Marble Co. v. Highway Board, supra, and follow
    the Federal practice of looking to the evide
  analysis:
    Analyses of the data's composition, document statistics,
    language/dialectal coverage, topical distribution, and biases are
    conducted are conducted in the paper
    [[The Pile Paper]](https://arxiv.org/pdf/2101.00027.pdf).
  # Construction
  dependencies: []
  license: >
    The Pile uses the MIT License, allowing proprietary or open source use
    on the condition that the terms of the license as well as a copyright
    notice [[MIT License]](https://arxiv.org/pdf/2201.07311.pdf).
  included: >
    The Pile data come from 22 sources, with over half of the data being from
    Common Crawl (Pile-CC; 227GB), fiction and nonfiction books (Books3; 101GB),
    biomedical articles (PubMed Central; 90GB), and code (Github; 95 GB).
    Refer to the paper for full decomposition
    [[Table 1]](https://arxiv.org/pdf/2101.00027.pdf#table.caption.2).
  excluded: >
    Authors report that they have excluded some datasets "because they were too
    small to be worth spending time or because the English component of the data
    did not merit inclusion on its own. Three datasets were excluded for other
    reasons: (1) US Congressional Records were excluded because it "reflects the
    opinions and biases of the political class over the past 200 years,
    including segregationism and xenophobia." (2) Online Fanfiction resources
    amounting to Hundreds of GiB were excluded on logistical grounds.
    (3) Literotica, platform where users can upload short-form erotic fiction,
    was excluded because the authors decided to exclude fanfiction, the
    corpus would require significant investigation, and corpus contain
    significant amount of stereotyping
    [[Appendix B]](https://arxiv.org/pdf/2101.00027.pdf).
  quality_control: >
    In addition to the data inclusion and exclusion decisions, the quality was
    controlled through filtering for English (pycld2 language classifier),
    filtering for documents similar to OpenWebText2 (classifier on CommonCrawl),
    and several forms of deduplication as detailed in the paper
    [[Appendix C]](https://arxiv.org/pdf/2101.00027.pdf#appendix.1.C)
    [[Appendix D]](https://arxiv.org/pdf/2101.00027.pdf#appendix.1.D).
  # Downstream
  access: >
    Unlimited public access; The dataset is freely available to the public and
    can be downloaded from The Eye
    [[The Pile]](https://mystic.the-eye.eu/public/AI/pile/).
  intended_uses: >
    The Pile was intended to be used as a high quality large text dataset for
    language modeling tasks, explained in more detail in the paper
    [[Section 1]](https://arxiv.org/pdf/2101.00027.pdf#section.1).
  prohibited_uses: >
    None
  monitoring: >
    None
  feedback: >
    Feedback can be given by emailing the authors at contact at eleuther.ai.

- type: model
  name: GPT-NeoX-20B
  # General
  organization: EleutherAI
  description: >
    GPT-NeoX-20B is an open-sourced autoregressive large language model.
  created_date: "2022-02-02"
  url: http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf
  model_card: https://mystic.the-eye.eu/public/AI/models/GPT-NeoX-20B/20B_model_card.md
  modality: Text (English) and Code
  size: 20B parameters (dense model)
  analysis: >
    The model was evaluated on standard NLP benchmarks: LAMBADA, ANLI,
    HellaSwag, MMLU among others
    [[Section 4]](http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf#section.4).
  # Construction
  dependencies:
    - The Pile
  training_emissions: >
    31.73 tCO2e; The amount of emission during the development and training of
    the model based on the author's estimation
    [[Section 6.4]](http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf#subsection.6.4).
  training_time: >
    1830 hours; Training time as reported by the authors
    [[Section 6.4]](http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf#subsection.6.4).
  training_hardware: >
    12 x 8 A100 GPUs; As outline by the authors
    [[Section 2.3]](http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf#subsection.2.3)
  quality_control: >
    None
  # Downstream
  access: >
    Unlimited public access; The model can be downloaded for free The Eye
    [[GPT-NeoX-20B]](https://mystic.the-eye.eu/public/AI/models/GPT-NeoX-20B/).
  license: >
    Apache 2.0; As indicated in the accompanying blog post
    [[EleutherAI Blog Post]](https://blog.eleuther.ai/announcing-20b/).
  intended_uses: >
    As stated in the model card: "GPT-NeoX-20B learns an inner representation
    of the English language that can be used to extract features useful for
    downstream tasks. The model is best at what it was pretrained for however,
    which is generating text from a prompt.
    Due to the generality of the pretraining set, it has acquired the ability
    to generate completions across a wide range of tasks - from programming to
    fiction writing
    [[Model Card]](https://mystic.the-eye.eu/public/AI/models/GPT-NeoX-20B/20B_model_card.md).
  prohibited_uses: >
    None
  monitoring: >
    None
  feedback: >
    Feedback can be provided using the #20b channel in EleutherAI Discord
    group
    [[EleutherAI Blog Post]](https://blog.eleuther.ai/announcing-20b/).
    Find the Discord link in the FAQ page
    [[FAQ]](https://www.eleuther.ai/faq/).
