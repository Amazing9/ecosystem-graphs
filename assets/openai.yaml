---

# Datasets

- type: dataset
  name: GPT-3 dataset
  # General
  organization: OpenAI
  description: >
    The GPT-3 dataset is the text corpus that was used to train the GPT-3
    model. The dataset isn't released to the public, and the information on it
    is limited to those provided in a section in the paper introducing the
    GPT-3 model
    [[Section 2.2]](https://arxiv.org/pdf/2005.14165.pdf#subsection.2.2).
  created_date: >
    2020-06-11; The GPT-3 dataset doesn't have a release date specified, but
    the acompanying model, GPT-3 was released to the public as part of a
    blog post introducing the OpenAI API at the date specified
    [[Open AI Blog Post]](https://openai.com/blog/openai-api/).
  url: https://arxiv.org/pdf/2005.14165.pdf
  datasheet: none
  modality: Text (English)
  size: 570 GB
  sample: []
  analysis: >
    The GPT-3 paper, which also introduces the GPT-3 dataset, provide a limited
    analysis on the GPT-3 dataset, reporting the dirtiness of the dataset after
    the it was filtered for text occuring in common benchmarking tasks.
    The authors report that "as the dataset becomes more contaminated, the
    variance of the clean/all fraction increases, but there is no apparent bias
    towards improved or degraded performance"
    [[Appendix C]](https://arxiv.org/pdf/2005.14165.pdf#appendix.C).
  # Construction
  dependencies: []
  license: >
    unknown; There is no known license specific to the GPT-3 dataset, however,
    the governing organization, OpenAI, licensed GPT-3 to Microsoft, which
    makes it likely that the GPT-3 dataset was also licensed
    [[OpenAI Blog Post]]
    (https://openai.com/blog/openai-licenses-gpt-3-technology-to-microsoft/).
  included: >
    The GPT-3 dataset is a combination of several corpora that are commonly
    used for NLP model training tasks, which are as follows, along with their
    representation percentage in the GPT-3 dataset: Common Crawl
    (filtered, 60%), WebText2 (22%), Books1 (8%), Books2 (8%), Wikipedia (3%)
    [[Section 2.2]](https://arxiv.org/pdf/2005.14165.pdf#subsection.2.2).
  excluded: >
    Low quality documents in the Common Crawl dataset, which are less like
    Wikipedia, Webtext or Books corpora, were filtered out using automatic
    filters and not included in the GPT-3 dataset
    [[Appendix A]](https://arxiv.org/pdf/2005.14165.pdf#appendix.A).
  quality_control: >
    In addition to excluding low quality documents from the Common Crawl
    dataset, the authors fuzzily deduplicated documents within each dataset, by
    removing documents that have high overlap with each other. The same
    procedure was followed to fuzzily deduplicate WebText from Common Crawl
    [[Appendix A]](https://arxiv.org/pdf/2005.14165.pdf#appendix.A).
    Text occuring in benchmark datasets were also partially removed
    [[Appendix C]](https://arxiv.org/pdf/2005.14165.pdf#appendix.C).
  # Downstream
  access: >
    The GPT-3 dataset isn't released to the public, but it may be available
    to Microsoft through the GPT-3 licencing agreement between OpenAI and
    Microsoft [[OpenAI Blog Post]]
    (https://openai.com/blog/openai-licenses-gpt-3-technology-to-microsoft/).
  intended_uses: >
    The intended use of the GPT-3 dataset was to train GPT-3. Since the dataset
    wasn't released to the public, we think that it wasn't meant to be a
    general purpose language model training dataset.
  prohibited_uses: >
    unknown; OpenAI didn't provide a list of prohibited uses specifically for
    the GPT-3 dataset. However, public OpenAI products are governed by the
    OpenAI Terms of Use, which can also apply to the OpenAI dataset.
    The OpenAI Terms of Use prohibit the following:
    (i) Illegal activities, such as child pornography, gambling, cybercrime,
    piracy, violating copyright, trademark or other intellectual property laws;
    (ii) Accessing or authorizing anyone to access the APIs from an embargoed
    country, region, or territory as prohibited by the U.S. government;
    (iii) Threatening, stalking, defaming, defrauding, degrading, victimizing
    or intimidating anyone for any reason
    [[Open AI Terms of Use]](https://openai.com/api/policies/terms/).
  monitoring: >
    unknown; There are no known monitoring mechanisms are in place for the use
    of the GPT-3 dataset.
  feedback: >
    unknown; There are no known feedback mechanisms for the GPT-3 dataset.

- type: dataset
  name: Codex dataset
  # General
  organization: OpenAI
  created_date: none
  url: https://arxiv.org/pdf/2107.03374.pdf
  datasheet: none
  modality: text (English)
  size: 159GB
  sample: []
  analysis: See the paper (Section 3.1).
  # Construction
  dependencies: []
  license: none
  included: GitHub
  excluded: autogenerated files; files with average line length > 100, maximum line length > 1000, or few alphanumeric characters
  quality_control: unknown
  # Downstream
  access: none
  intended_uses: Training language models.
  prohibited_uses: none
  monitoring: none
  feedback: none

# Models

- type: model
  name: GPT-3
  # General
  organization: OpenAI
  created_date: "2020-06-11"
  url: https://arxiv.org/pdf/2005.14165.pdf
  model_card: https://github.com/openai/gpt-3/blob/master/model-card.md
  modality: text (English)
  size: Autoregressive Transformer with 175B parameters
  analysis: Evaluated on language modeling, closed-book question answering, translation, Winograd-style tasks, commonsense reasoning, reading comprehension, SuperGLUE, NLI, synthetic tasks, generation
  # Construction
  dependencies:
  - GPT-3 dataset
  training_emissions: 552.1 tCO2e (estimate from https://arxiv.org/abs/2104.10350)
  training_time: 3.64E+03 PF-days
  training_hardware: Trained on [Azure supercomputer](https://blogs.microsoft.com/ai/openai-azure-supercomputer/), which has 10000 GPUs with 400 Gbps
  quality_control: The model is not released.
  # Downstream
  access: Open to anyone in [supported countries](https://beta.openai.com/docs/supported-countries) via [API access](https://openai.com/api/); full model access to Microsoft
  license: exclusive license to Microsoft
  intended_uses: unknown
  prohibited_uses: OpenAI prohibits the use of the model to generate restricted content per their [content policy](https://beta.openai.com/docs/usage-guidelines/content-policy). Other disallowed applications are listed in the [disallowed applications](https://beta.openai.com/docs/usage-guidelines/disallowed-applications) list.
  monitoring: OpenAI reviews all use cases of the model.
  feedback: Feedback form is [here](https://docs.google.com/forms/d/e/1FAIpQLSf1-7D2eiOcxz9KJSQ0Ie_R6ST_TsNXOvJE9MUyGax1NYkMfQ/viewform)

- type: model
  name: Codex
  # General
  organization: OpenAI
  created_date: "2021-08-10"
  url: https://arxiv.org/pdf/2107.03374.pdf
  model_card: none
  modality: text (English and code)
  size: 12B parameters, supervised fine-tuning
  analysis: Evaluated on HumanEval using pass@k and BLEU scores; see the paper (Section 2).
  # Construction
  dependencies:
  - GPT-3
  - Codex dataset
  training_emissions: unknown
  training_time: Authors estimate hundreds of petaflop/s-days of compute; see the paper (Section 7.6).
  training_hardware: Azure
  quality_control: The model is not released; see the paper for legal implications (Section 7.7) and risk mitigation (Section 7.8).
  # Downstream
  access: Available via the [OpenAI API](https://openai.com/api/); full model access to Microsoft for [GitHub Copilot](https://copilot.github.com/).
  license: Exclusive license to Microsoft.
  intended_uses: Coding assistant.
  prohibited_uses: unknown
  monitoring: none
  feedback: none

- type: model
  name: InstructGPT
  # General
  organization: OpenAI
  created_date: "2022-01-27"
  url: https://openai.com/blog/instruction-following/
  model_card: https://github.com/openai/following-instructions-human-feedback/blob/main/model-card.md
  modality: text (English and code)
  size: 175B parameters, fine-tuned with supervised fine-tuning, reward modeling, reinforcement learning
  analysis: https://arxiv.org/abs/2203.02155
  # Construction
  dependencies:
  - GPT-3
  training_emissions: unknown
  training_time: 175B SFT model required 4.9 petaflops/s-days; 175B PPO-ptx model required 60 petaflops/s-days
  training_hardware: unknown
  quality_control: The model is not released.
  # Downstream
  access: Available via the [OpenAI API](https://openai.com/api/)
  license: Exclusive license to Microsoft
  intended_uses: Performing tasks through instructions in plain language.
  prohibited_uses: OpenAI prohibits the use of the model to generate restricted content per their [content policy](https://beta.openai.com/docs/usage-guidelines/content-policy). Other disallowed applications are listed in the [disallowed applications](https://beta.openai.com/docs/usage-guidelines/disallowed-applications) list.
  monitoring: OpenAI reviews all use cases of the model.
  feedback: none

- type: application
  name: OpenAPI API
  # General
  organization: OpenAI
  created_date: "2020-06-11"
  url: https://openai.com/api/
  # Construction
  dependencies:
  - GPT-3
  - Codex
  - InstructGPT
  adaptation: none?
  output_space: text completions, log probabilities, and embeddings
  quality_control: TODO
  # Downstream
  access: Available to everyone excluding those in [embargoed countries](https://beta.openai.com/docs/supported-countries)
  license: Section 2a of [Terms of Use](https://openai.com/api/policies/terms/)
  terms_of_service: https://openai.com/api/policies/terms/
  intended_uses: TODO
  prohibited_uses: illegal activities (child pornography, gambling, cybercrime, piracy, violating copyright, trademark or other intellectual property laws) and threatening, stalking, defaming, defrauding, degrading, victimizing or intimidating anyone [terms of service](https://openai.com/api/policies/terms/) API cannot be used to generate these kinds of content hate speech, harassment, violence, self-harm, adult, political, spam, deception, malware [Usage Guidelines](https://beta.openai.com/docs/usage-guidelines)
  monitoring: Use of API is monitored, per Section 5b of [Terms of Use](https://openai.com/api/policies/terms/)
  feedback: private feedback at support @ openai.com
  # Deployment
  monthly_active_users: unknown
  user_distribution: unknown
  failures: unknown

- type: application
  name: Duolingo
  # General
  organization: Duolingo
  created_date: "2020-06-11"
  url: https://openai.com/api/
  # Construction
  dependencies:
  - GPT-3
  adaptation: unknown
  output_space: French grammar corrections
  quality_control: unknown
  # Downstream
  access: none
  license: none
  terms_of_service: https://www.duolingo.com/terms
  intended_uses: none
  prohibited_uses: none
  monitoring: unknown
  feedback: none
  # Deployment
  monthly_active_users: 42 million (all languages)
  user_distribution: unknown
  failures: unknown

- type: application
  name: Viable
  # General
  organization: Viable
  created_date: unknown
  url: https://www.askviable.com/
  # Construction
  dependencies:
  - GPT-3
  adaptation: none
  output_space: question and answer, summarization, sentiment analysis,topic identification
  quality_control: unknown
  # Downstream
  access: none
  license: none
  terms_of_service: https://www.askviable.com/terms-of-service
  intended_uses: none
  prohibited_uses: none
  monitoring: unknown
  feedback: unknown
  # Deployment
  monthly_active_users: unknown
  user_distribution: unknown
  failures: unknown

- type: application
  name: HyperWrite
  # General
  organization: OthersideAI
  created_date: unknown
  url: https://hyperwriteai.com/
  # Construction
  dependencies:
  - GPT-3
  adaptation: none
  output_space: generation
  quality_control: unknown
  # Downstream
  access: none
  license: none
  terms_of_service: https://hyperwriteai.com/terms
  intended_uses: none
  prohibited_uses: none
  monitoring: unknown
  feedback: unknown
  # Deployment
  monthly_active_users: unknown
  user_distribution: unknown
  failures: unknown
